{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Pré-processamento dos dados textuais\n",
    "\n",
    "## Transformando os dados em um dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./datascience_env/lib/python3.8/site-packages (1.4.0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./datascience_env/lib/python3.8/site-packages (from pandas) (2021.3)\r\n",
      "Requirement already satisfied: numpy>=1.18.5 in ./datascience_env/lib/python3.8/site-packages (from pandas) (1.22.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./datascience_env/lib/python3.8/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in ./datascience_env/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\r\n",
      "Requirement already satisfied: unidecode in ./datascience_env/lib/python3.8/site-packages (1.3.2)\r\n",
      "Requirement already satisfied: nltk in ./datascience_env/lib/python3.8/site-packages (3.6.7)\r\n",
      "Requirement already satisfied: click in ./datascience_env/lib/python3.8/site-packages (from nltk) (8.0.3)\r\n",
      "Requirement already satisfied: tqdm in ./datascience_env/lib/python3.8/site-packages (from nltk) (4.62.3)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./datascience_env/lib/python3.8/site-packages (from nltk) (2022.1.18)\r\n",
      "Requirement already satisfied: joblib in ./datascience_env/lib/python3.8/site-packages (from nltk) (1.1.0)\r\n",
      "Requirement already satisfied: sklearn in ./datascience_env/lib/python3.8/site-packages (0.0)\r\n",
      "Requirement already satisfied: scikit-learn in ./datascience_env/lib/python3.8/site-packages (from sklearn) (1.0.2)\r\n",
      "Requirement already satisfied: numpy>=1.14.6 in ./datascience_env/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.22.1)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in ./datascience_env/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.7.3)\r\n",
      "Requirement already satisfied: joblib>=0.11 in ./datascience_env/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.1.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./datascience_env/lib/python3.8/site-packages (from scikit-learn->sklearn) (3.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install unidecode\n",
    "!pip install nltk\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_base = pd.read_json('goodreads_books.json')\n",
    "df_mini = df_base.sample(n=1000, random_state=1)\n",
    "df_mini.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Limpeza dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import unidecode\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/igor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/igor/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def remove_specials(tokens):\n",
    "    return [unidecode.unidecode(word) for word in tokens]\n",
    "\n",
    "def remove_punctuation(tokens):\n",
    "    table = str.maketrans(\"\",\"\",string.punctuation)\n",
    "    return [w.translate(table) for w in tokens]\n",
    "\n",
    "def array_lower(tokens):\n",
    "    return [w.lower() for w in tokens]\n",
    "\n",
    "def remove_no_words(tokens):\n",
    "    return [word for word in tokens if word.isalpha()]\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [w for w in tokens if not w in stop_words]\n",
    "\n",
    "def text_clean(df_text_column, log=False):\n",
    "    if log: print(\"Starting\")\n",
    "\n",
    "    # Dividindo texto em tokens\n",
    "    df_text_column = df_text_column.apply(word_tokenize)\n",
    "\n",
    "    if log: print(\"Tokens split done\")  # Some log\n",
    "\n",
    "    # Convertendo texto para minusculo\n",
    "    df_text_column = df_text_column.apply(array_lower)\n",
    "\n",
    "    if log: print(\"Upper letters converted to normal\")\n",
    "\n",
    "    # Removendo pontuação\n",
    "    df_text_column = df_text_column.apply(remove_punctuation)\n",
    "\n",
    "    if log: print(\"Removed punctiation\")\n",
    "\n",
    "    # Removendo caracteres especiais\n",
    "    df_text_column = df_text_column.apply(remove_specials)\n",
    "\n",
    "    if log: print(\"Specials chars removed\")\n",
    "\n",
    "    # Removendo tokens que não sao palavras\n",
    "    df_text_column = df_text_column.apply(remove_no_words)\n",
    "\n",
    "    if log: print(\"Removed non-words tokens\")\n",
    "\n",
    "    # Removendo tokens que são stop words\n",
    "    df_text_column = df_text_column.apply(remove_stop_words)\n",
    "\n",
    "    if log: print(\"Removed tokens that are stop-words\\nFinished\")\n",
    "\n",
    "    return df_text_column\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Auxiliar log function\n",
    "def size_df(d):\n",
    "    print(\"Size=\", d.memory_usage(deep=True).sum()/10**9, \"GB\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         book_id                                        description  \\\n51285   27833724  [tang, tu, nigao, bai, shiteshimatsutashan, ti...   \n615323  24754076  [sixteen, yearold, madeline, struggled, epilep...   \n118404   8737174  [epic, story, thomas, caleintroduced, memorabl...   \n574805  15831501  [father, thought, business, came, first, daugh...   \n369607    892602  [last, night, dreamt, went, manderley, novel, ...   \n\n                         genre language_code  \n51285           comics_graphic           eng  \n615323             young_adult           eng  \n118404      fantasy_paranormal           eng  \n574805                 romance           eng  \n369607  mystery_thriller_crime           eng  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>book_id</th>\n      <th>description</th>\n      <th>genre</th>\n      <th>language_code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>51285</th>\n      <td>27833724</td>\n      <td>[tang, tu, nigao, bai, shiteshimatsutashan, ti...</td>\n      <td>comics_graphic</td>\n      <td>eng</td>\n    </tr>\n    <tr>\n      <th>615323</th>\n      <td>24754076</td>\n      <td>[sixteen, yearold, madeline, struggled, epilep...</td>\n      <td>young_adult</td>\n      <td>eng</td>\n    </tr>\n    <tr>\n      <th>118404</th>\n      <td>8737174</td>\n      <td>[epic, story, thomas, caleintroduced, memorabl...</td>\n      <td>fantasy_paranormal</td>\n      <td>eng</td>\n    </tr>\n    <tr>\n      <th>574805</th>\n      <td>15831501</td>\n      <td>[father, thought, business, came, first, daugh...</td>\n      <td>romance</td>\n      <td>eng</td>\n    </tr>\n    <tr>\n      <th>369607</th>\n      <td>892602</td>\n      <td>[last, night, dreamt, went, manderley, novel, ...</td>\n      <td>mystery_thriller_crime</td>\n      <td>eng</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw_df = df_mini.copy()\n",
    "# df_mini['description'] = text_clean(df_mini['description'])\n",
    "# df_mini.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'Tang Tu niGao Bai shiteshimatsutaShan Tian . soreniDui suruuraranoFan Shi haJing kubekimonodatsuta!? uraranoFan Shi woWen itaShan Tian ha, Bi Nu noJi Yi woLi sukotowoJue Yi !! demo, Ji Yi woLi suFang Fa gasatsupariwakannee!! Shan Tian ha, hitomazuXiao Tian Qie noNeng Li woJu u....ga!'"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_base.loc[51285, 'description'] # Noisy data"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stemming"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "         book_id                                        description  \\\n51285   27833724  [tang, tu, nigao, bai, shiteshimatsutashan, ti...   \n615323  24754076  [sixteen, yearold, madelin, struggl, epilepsi,...   \n118404   8737174  [epic, stori, thoma, caleintroduc, memor, left...   \n574805  15831501  [father, thought, busi, came, first, daughter,...   \n369607    892602  [last, night, dreamt, went, manderley, novel, ...   \n\n                         genre language_code  \n51285           comics_graphic           eng  \n615323             young_adult           eng  \n118404      fantasy_paranormal           eng  \n574805                 romance           eng  \n369607  mystery_thriller_crime           eng  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>book_id</th>\n      <th>description</th>\n      <th>genre</th>\n      <th>language_code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>51285</th>\n      <td>27833724</td>\n      <td>[tang, tu, nigao, bai, shiteshimatsutashan, ti...</td>\n      <td>comics_graphic</td>\n      <td>eng</td>\n    </tr>\n    <tr>\n      <th>615323</th>\n      <td>24754076</td>\n      <td>[sixteen, yearold, madelin, struggl, epilepsi,...</td>\n      <td>young_adult</td>\n      <td>eng</td>\n    </tr>\n    <tr>\n      <th>118404</th>\n      <td>8737174</td>\n      <td>[epic, stori, thoma, caleintroduc, memor, left...</td>\n      <td>fantasy_paranormal</td>\n      <td>eng</td>\n    </tr>\n    <tr>\n      <th>574805</th>\n      <td>15831501</td>\n      <td>[father, thought, busi, came, first, daughter,...</td>\n      <td>romance</td>\n      <td>eng</td>\n    </tr>\n    <tr>\n      <th>369607</th>\n      <td>892602</td>\n      <td>[last, night, dreamt, went, manderley, novel, ...</td>\n      <td>mystery_thriller_crime</td>\n      <td>eng</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "stemmized = lambda d: [porter.stem(tk) for tk in d]\n",
    "df_mini['description'] = df_mini['description'].apply(stemmized)\n",
    "df_mini.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TFIDF\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using the hand made preprocessor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "do_nothing = lambda x: x\n",
    "vect_manual = TfidfVectorizer(tokenizer=do_nothing, lowercase=False, preprocessor=do_nothing)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using only the class from sklearn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(strip_accents='unicode', stop_words='english', ngram_range=(1, 1))\n",
    "freq = vect.fit_transform(df_mini['description'].values.tolist())\n",
    "df_freq = pd.DataFrame(data=freq.todense(), columns=vect.get_feature_names_out())\n",
    "df_freq.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "dsenv",
   "language": "python",
   "display_name": "Python datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}